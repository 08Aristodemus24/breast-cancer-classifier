{"layer_1": 201, "layer_2": 901, "layer_3": 101, "layer_4": 601, "activation": "relu", "learning_rate": 0.003, "lambda": 0.01}