{"layer_1": 401, "layer_2": 201, "layer_3": 901, "activation": "relu", "learning_rate": 0.003, "lambda": 0.01}