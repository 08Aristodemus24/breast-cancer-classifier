{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model training\n",
    "- this notebook will serve to extract the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy as bce_loss\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, BinaryCrossentropy as bce_metric\n",
    "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform, RandomNormal, RandomUniform, HeNormal, HeUniform\n",
    "from tensorflow.keras.optimizers import Adadelta, Adafactor, Adagrad, Adam, AdamW, Adamax, Ftrl, Nadam, RMSprop, SGD \n",
    "\n",
    "from utilities.data_preprocessor import preprocess\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architecture and hyper parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"\n",
    "    hp - hyperparameter\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    hp_kernel_initializer = hp.Choice('initializer', values=['GlorotNormal', 'GlorotUniform', 'RandomNormal', 'RandomUniform', 'HeNormal', 'HeUniform'])\n",
    "    initializers = {\n",
    "        'GlorotNormal': GlorotNormal(),\n",
    "        'GlorotUniform': GlorotUniform(),\n",
    "        'RandomNormal': RandomNormal(mean=0.0, stddev=1.0),\n",
    "        'RandomUniform': RandomUniform(minval=-0.05, maxval=0.05),\n",
    "        'HeNormal': HeNormal(),\n",
    "        'HeUniform': HeUniform()\n",
    "    }\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "\n",
    "    # the drop probability values, instead of keep probability\n",
    "    hp_dropout = hp.Choice('dropout', values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    # learning rate alpha\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1.2, 0.03, 0.01, 0.0075, 0.003, 0.001,])\n",
    "\n",
    "    # regularization value lambda\n",
    "    hp_lambda = hp.Choice('lambda', values=[10.0, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.25, 0.125, 0.01,])\n",
    "    # hp_dropout = hp.Choice('dropout', value=[0.8, 0.85, 0.7, 0.6])\n",
    "\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['Adadelta', 'Adafactor', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop', 'SGD'])\n",
    "    optimizers = {\n",
    "        'Adadelta': Adadelta(learning_rate=hp_learning_rate),\n",
    "        'Adafactor': Adafactor(learning_rate=hp_learning_rate),\n",
    "        'Adagrad': Adagrad(learning_rate=hp_learning_rate),\n",
    "        'Adam': Adam(learning_rate=hp_learning_rate),\n",
    "        'AdamW': AdamW(learning_rate=hp_learning_rate),\n",
    "        'Adamax': Adamax(learning_rate=hp_learning_rate), \n",
    "        'Ftrl': Ftrl(learning_rate=hp_learning_rate),\n",
    "        'Nadam': Nadam(learning_rate=hp_learning_rate),\n",
    "        'RMSprop': RMSprop(learning_rate=hp_learning_rate),\n",
    "        'SGD': SGD(learning_rate=hp_learning_rate)\n",
    "    }\n",
    "\n",
    "    # number of hidden layers\n",
    "    for index, l in enumerate(range(hp.Int('layer_num', min_value=1, max_value=80))):\n",
    "        # number of nodes per layer\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(f'layer_{index + 1}', min_value=1, max_value=1000, step=100), \n",
    "            activation=hp_activation, \n",
    "            kernel_initializer=initializers[hp_kernel_initializer],\n",
    "            kernel_regularizer=L2(hp_lambda)))\n",
    "        \n",
    "        model.add(Dropout(hp_dropout))\n",
    "\n",
    "    model.add(Dense(units=1, activation='linear', kernel_regularizer=L2(hp_lambda)))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizers[hp_optimizer],\n",
    "        loss=bce_loss(from_logits=True),\n",
    "        metrics=[bce_metric(), BinaryAccuracy(threshold=0.5)]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuner\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder, \n",
    "    objective=[kt.Objective('val_binary_accuracy', 'max'), kt.Objective('val_binary_crossentropy', 'min')], \n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='tuned_models',\n",
    "    project_name='model'\n",
    ")\n",
    "\n",
    "# if cross validation loss does not improve after 10 \n",
    "# consecutive epochs we stop training our modelearly\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with the selected features to use\n",
    "df = pd.read_csv('./data.csv')\n",
    "X, Y = preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>2.217515</td>\n",
       "      <td>2.255747</td>\n",
       "      <td>2.489734</td>\n",
       "      <td>-0.565265</td>\n",
       "      <td>2.833031</td>\n",
       "      <td>2.487578</td>\n",
       "      <td>-0.214002</td>\n",
       "      <td>1.316862</td>\n",
       "      <td>0.724026</td>\n",
       "      <td>0.660820</td>\n",
       "      <td>1.148757</td>\n",
       "      <td>0.907083</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>0.499255</td>\n",
       "      <td>-0.876244</td>\n",
       "      <td>0.263327</td>\n",
       "      <td>0.742402</td>\n",
       "      <td>-0.605351</td>\n",
       "      <td>-0.692926</td>\n",
       "      <td>-0.440780</td>\n",
       "      <td>0.260162</td>\n",
       "      <td>-0.805450</td>\n",
       "      <td>-0.099444</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>-0.398008</td>\n",
       "      <td>1.228676</td>\n",
       "      <td>-0.780083</td>\n",
       "      <td>0.850928</td>\n",
       "      <td>1.181336</td>\n",
       "      <td>-0.297005</td>\n",
       "      <td>0.814974</td>\n",
       "      <td>0.213076</td>\n",
       "      <td>1.424827</td>\n",
       "      <td>0.237036</td>\n",
       "      <td>0.293559</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>2.867383</td>\n",
       "      <td>4.910919</td>\n",
       "      <td>0.326373</td>\n",
       "      <td>-0.110409</td>\n",
       "      <td>0.286593</td>\n",
       "      <td>-0.288378</td>\n",
       "      <td>0.689702</td>\n",
       "      <td>2.744280</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>1.115007</td>\n",
       "      <td>4.732680</td>\n",
       "      <td>2.047511</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.562450</td>\n",
       "      <td>1.270543</td>\n",
       "      <td>-0.790244</td>\n",
       "      <td>1.273189</td>\n",
       "      <td>1.190357</td>\n",
       "      <td>1.483067</td>\n",
       "      <td>-0.048520</td>\n",
       "      <td>0.828471</td>\n",
       "      <td>1.144205</td>\n",
       "      <td>-0.361092</td>\n",
       "      <td>0.499328</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>2.110995</td>\n",
       "      <td>0.721473</td>\n",
       "      <td>2.060786</td>\n",
       "      <td>2.343856</td>\n",
       "      <td>1.041842</td>\n",
       "      <td>0.219060</td>\n",
       "      <td>1.947285</td>\n",
       "      <td>2.320965</td>\n",
       "      <td>-0.312589</td>\n",
       "      <td>-0.931027</td>\n",
       "      <td>2.782080</td>\n",
       "      <td>0.071025</td>\n",
       "      <td>2.379583</td>\n",
       "      <td>2.604187</td>\n",
       "      <td>1.086384</td>\n",
       "      <td>0.191805</td>\n",
       "      <td>0.666001</td>\n",
       "      <td>2.067178</td>\n",
       "      <td>-1.138416</td>\n",
       "      <td>0.167980</td>\n",
       "      <td>1.901185</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>1.752563</td>\n",
       "      <td>2.015301</td>\n",
       "      <td>0.378365</td>\n",
       "      <td>-0.273318</td>\n",
       "      <td>0.664512</td>\n",
       "      <td>1.629151</td>\n",
       "      <td>-1.360158</td>\n",
       "      <td>-0.709091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>1.704854</td>\n",
       "      <td>2.085134</td>\n",
       "      <td>1.615931</td>\n",
       "      <td>1.723842</td>\n",
       "      <td>0.102458</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>0.693043</td>\n",
       "      <td>1.263669</td>\n",
       "      <td>-0.217664</td>\n",
       "      <td>-1.058611</td>\n",
       "      <td>1.300499</td>\n",
       "      <td>2.260938</td>\n",
       "      <td>1.156857</td>\n",
       "      <td>1.291565</td>\n",
       "      <td>-0.424010</td>\n",
       "      <td>-0.069758</td>\n",
       "      <td>0.252202</td>\n",
       "      <td>0.808431</td>\n",
       "      <td>-0.189161</td>\n",
       "      <td>-0.490556</td>\n",
       "      <td>1.536720</td>\n",
       "      <td>2.047399</td>\n",
       "      <td>1.421940</td>\n",
       "      <td>1.494959</td>\n",
       "      <td>-0.691230</td>\n",
       "      <td>-0.394820</td>\n",
       "      <td>0.236573</td>\n",
       "      <td>0.733827</td>\n",
       "      <td>-0.531855</td>\n",
       "      <td>-0.973978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.702284</td>\n",
       "      <td>2.045574</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.577953</td>\n",
       "      <td>-0.840484</td>\n",
       "      <td>-0.038680</td>\n",
       "      <td>0.046588</td>\n",
       "      <td>0.105777</td>\n",
       "      <td>-0.809117</td>\n",
       "      <td>-0.895587</td>\n",
       "      <td>0.184892</td>\n",
       "      <td>-0.257371</td>\n",
       "      <td>0.276693</td>\n",
       "      <td>0.180698</td>\n",
       "      <td>-0.379342</td>\n",
       "      <td>0.661277</td>\n",
       "      <td>0.510827</td>\n",
       "      <td>0.612157</td>\n",
       "      <td>-0.891416</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>0.561361</td>\n",
       "      <td>1.374854</td>\n",
       "      <td>0.579001</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>-0.809587</td>\n",
       "      <td>0.350735</td>\n",
       "      <td>0.326767</td>\n",
       "      <td>0.414069</td>\n",
       "      <td>-1.104549</td>\n",
       "      <td>-0.318409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1.838341</td>\n",
       "      <td>2.336457</td>\n",
       "      <td>1.982524</td>\n",
       "      <td>1.735218</td>\n",
       "      <td>1.525767</td>\n",
       "      <td>3.272144</td>\n",
       "      <td>3.296944</td>\n",
       "      <td>2.658866</td>\n",
       "      <td>2.137194</td>\n",
       "      <td>1.043695</td>\n",
       "      <td>1.157935</td>\n",
       "      <td>0.686088</td>\n",
       "      <td>1.438530</td>\n",
       "      <td>1.009503</td>\n",
       "      <td>-0.173000</td>\n",
       "      <td>2.017716</td>\n",
       "      <td>1.302285</td>\n",
       "      <td>0.785721</td>\n",
       "      <td>0.326634</td>\n",
       "      <td>0.904057</td>\n",
       "      <td>1.961239</td>\n",
       "      <td>2.237926</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>1.653171</td>\n",
       "      <td>1.430427</td>\n",
       "      <td>3.904848</td>\n",
       "      <td>3.197605</td>\n",
       "      <td>2.289985</td>\n",
       "      <td>1.919083</td>\n",
       "      <td>2.219635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>-1.808401</td>\n",
       "      <td>1.221792</td>\n",
       "      <td>-1.814389</td>\n",
       "      <td>-1.347789</td>\n",
       "      <td>-3.112085</td>\n",
       "      <td>-1.150752</td>\n",
       "      <td>-1.114873</td>\n",
       "      <td>-1.261820</td>\n",
       "      <td>-0.820070</td>\n",
       "      <td>-0.561032</td>\n",
       "      <td>-0.070279</td>\n",
       "      <td>0.383092</td>\n",
       "      <td>-0.157449</td>\n",
       "      <td>-0.466152</td>\n",
       "      <td>0.049342</td>\n",
       "      <td>-1.163516</td>\n",
       "      <td>-1.057501</td>\n",
       "      <td>-1.913447</td>\n",
       "      <td>0.752830</td>\n",
       "      <td>-0.382754</td>\n",
       "      <td>-1.410893</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.432735</td>\n",
       "      <td>-1.075813</td>\n",
       "      <td>-1.859019</td>\n",
       "      <td>-1.207552</td>\n",
       "      <td>-1.305831</td>\n",
       "      <td>-1.745063</td>\n",
       "      <td>-0.048138</td>\n",
       "      <td>-0.751207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
       "0       1.097064     -2.073335  ...        2.750622                 1.937015\n",
       "1       1.829821     -0.353632  ...       -0.243890                 0.281190\n",
       "2       1.579888      0.456187  ...        1.152255                 0.201391\n",
       "3      -0.768909      0.253732  ...        6.046041                 4.935010\n",
       "4       1.750297     -1.151816  ...       -0.868353                -0.397100\n",
       "..           ...           ...  ...             ...                      ...\n",
       "564     2.110995      0.721473  ...       -1.360158                -0.709091\n",
       "565     1.704854      2.085134  ...       -0.531855                -0.973978\n",
       "566     0.702284      2.045574  ...       -1.104549                -0.318409\n",
       "567     1.838341      2.336457  ...        1.919083                 2.219635\n",
       "568    -1.808401      1.221792  ...       -0.048138                -0.751207\n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble train the multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 215 Complete [00h 00m 32s]\n",
      "multi_objective: 2.746040999889374\n",
      "\n",
      "Best multi_objective So Far: -0.9606144577264786\n",
      "Total elapsed time: 01h 35m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# fit model to data\n",
    "tuner.search(\n",
    "    X, Y, \n",
    "    epochs=50, \n",
    "    validation_split=0.3, \n",
    "    callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the hyper parameters of the best model that trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_best_hyperparameters()[0].get('layer_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0: 101\n",
      "layer_1: 801\n",
      "activation: tanh\n",
      "learning_rate: 0.0075\n",
      "lambda: 0.25\n",
      "optimizers: Nadam\n",
      "dropout: 0.3\n",
      "kernel_initializer: HeUniform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters at 0x1a129d389a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_names = [f\"layer_{l}\" for l in range(2)] + ['activation', 'learning_rate', 'lambda', 'optimizer', 'dropout', 'initializer']\n",
    "best_hyper_params = {}\n",
    "for hp in hp_names:\n",
    "    best_hyper_param = tuner.get_best_hyperparameters()[0].get(hp)\n",
    "    print(f'{hp}: {best_hyper_param}')\n",
    "\n",
    "    if hp not in best_hyper_params:\n",
    "        best_hyper_params[hp] = best_hyper_param\n",
    "\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "best_hps\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save best hyper parameter values to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layer_0': 101,\n",
       " 'layer_1': 801,\n",
       " 'activation': 'tanh',\n",
       " 'learning_rate': 0.0075,\n",
       " 'lambda': 0.25,\n",
       " 'optimizers': 'Nadam',\n",
       " 'dropout': 0.3,\n",
       " 'kernel_initializer': 'HeUniform'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "best_hyper_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/best_hyper_params.json', 'w') as out_file:\n",
    "    json.dump(best_hyper_params, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breast-cancer-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
