{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this notebook takes the results of the ant-colony-optimizer notebook\n",
    "- previous notebook will store the results of specifically the best ant which has the lowest cost\n",
    "- the resulting features/tour it has used will be used in this notebook\n",
    "- this notebook will train multiple neural network models using the best ants features\n",
    "- models will undergo constant hyper parameter tuning\n",
    "- the end extract the hyper parameters that produces the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.nn import sigmoid\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy as bce_loss\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, BinaryCrossentropy as bce_metric\n",
    "\n",
    "from utilities.data_preprocessor import preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architecture and hyper parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    \"\"\"\n",
    "    hp - hyperparameter\n",
    "    \"\"\"\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "\n",
    "    # number of nodes per layer\n",
    "    hp_layer_1 = hp.Int('layer_1', min_value=1, max_value=1000, step=100)\n",
    "    hp_layer_2 = hp.Int('layer_2', min_value=1, max_value=1000, step=100)\n",
    "    hp_layer_3 = hp.Int('layer_3', min_value=1, max_value=1000, step=100)\n",
    "\n",
    "    # learning rate alpha\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1.2, 0.03, 0.01, 0.0075, 0.003, 0.001,])\n",
    "\n",
    "    # regularization value lambda\n",
    "    hp_lambda = hp.Choice('lambda', values=[10.0, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.25, 0.125, 0.01,])\n",
    "    # hp_dropout = hp.Choice('dropout', value=[0.8, 0.85, 0.7, 0.6])\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(units=hp_layer_1, activation=hp_activation, kernel_regularizer=L2(hp_lambda)),\n",
    "        Dense(units=hp_layer_2, activation=hp_activation, kernel_regularizer=L2(hp_lambda)),\n",
    "        Dense(units=hp_layer_3, activation=hp_activation, kernel_regularizer=L2(hp_lambda)),\n",
    "        Dense(units=1, activation='linear', kernel_regularizer=L2(hp_lambda))\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss=bce_loss(from_logits=True),\n",
    "        metrics=[bce_metric(), BinaryAccuracy(threshold=0.5)]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuner\n",
    "tuner = kt.Hyperband(\n",
    "    model_builder, \n",
    "    objective=kt.Objective('val_binary_accuracy', 'max'), \n",
    "    max_epochs=100,\n",
    "    factor=3,\n",
    "    directory='tuned_models',\n",
    "    project_name='model'\n",
    ")\n",
    "\n",
    "# if cross validation loss does not improve after 10 \n",
    "# consecutive epochs we stop training our modelearly\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "- [20, 16, 19, 14, 13, 11, 5, 3, 9, 28, 24, 15, 17, 21, 10] is the path of the best ant\n",
    "- train multiple models with these feature indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, Y = preprocess(pd.read_csv('./data.csv'))\n",
    "\n",
    "# fit model to data\n",
    "tuner.search(\n",
    "    X, Y, \n",
    "    epochs=50, \n",
    "    validation_split=0.3, \n",
    "    callbacks=[stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyper parameters of \n",
    "# the best model that trained \n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# use the extracted best hyper params to build final model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X, Y,\n",
    "    epochs=50,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[stop_early]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
